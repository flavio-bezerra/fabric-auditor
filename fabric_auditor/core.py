import os
import re
import time
import json
import logging
import base64
import inspect
import requests
from typing import Optional, Tuple, Any

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("azure").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

class FabricAuditor:
    def __init__(self, llm_client: Optional[Any] = None, auto_install: bool = True):
        """
        Inicializa o FabricAuditor.
        
        Args:
            llm_client (optional): Um objeto cliente LLM instanciado. Se None, configura automaticamente.
            auto_install (bool): Se True, verifica e instala depend√™ncias ausentes automaticamente.
        """
        if auto_install:
            self._ensure_dependencies()

        if llm_client:
            self.llm_client = llm_client
        else:
            logger.info("Nenhum cliente LLM fornecido. Tentando configura√ß√£o autom√°tica padr√£o...")
            self.llm_client = self._setup_default_client()
        
        # Padr√µes para ignorar (auto-exclus√£o)
        self.ignore_patterns = [
            "def snapshot_notebook_limpo",
            "snapshot_notebook_limpo()",
            "FabricAuditor",
            "audit_code",
            "summarize_notebook",
            "mssparkutils.credentials.getToken",
            "trident.workspace.id",
            "# AUDIT_IGNORE"  # Marcador manual para ignorar c√©lulas
        ]

    def _ensure_dependencies(self):
        """Verifica e instala depend√™ncias cr√≠ticas se estiverem faltando."""
        required_packages = [
            ("azure.identity", "azure-identity"),
            ("azure.keyvault.secrets", "azure-keyvault-secrets"),
            ("openai", "openai")
        ]
        
        missing = []
        for import_name, install_name in required_packages:
            try:
                __import__(import_name)
            except ImportError:
                missing.append(install_name)
        
        if missing:
            print(f"üì¶ Depend√™ncias ausentes detectadas: {', '.join(missing)}")
            print("‚è≥ Instalando automaticamente... (Isso pode levar alguns instantes)")
            try:
                import subprocess
                import sys
                subprocess.check_call([sys.executable, "-m", "pip", "install"] + missing)
                print("‚úÖ Instala√ß√£o conclu√≠da! Nota: Se ocorrerem erros de importa√ß√£o, reinicie o kernel.")
            except Exception as e:
                logger.error(f"‚ùå Falha na instala√ß√£o autom√°tica: {e}")

    def _setup_default_client(self) -> Any:
        """
        Configura o cliente AzureOpenAI padr√£o lendo do JSON e Key Vault.
        """
        try:
            import notebookutils
            from azure.identity import ClientSecretCredential
            from azure.keyvault.secrets import SecretClient
            from openai import AzureOpenAI
            
            # 1. Ler Credenciais do Arquivo
            # Verifica se notebookutils tem nbResPath (algumas vers√µes podem variar)
            if not hasattr(notebookutils, 'nbResPath'):
                 # Tentativa de fallback para mssparkutils se necess√°rio, ou erro mais claro
                 pass 

            json_path = f"{notebookutils.nbResPath}/env/CS_API_REST_LOGIN.json"
            if not os.path.exists(json_path):
                raise FileNotFoundError(f"Arquivo de configura√ß√£o n√£o encontrado em: {json_path}")

            with open(json_path, encoding='utf-8') as arquivo:
                certificate = json.load(arquivo)

            # 2. Pegar segredo do Key Vault
            key_vault_url = "https://kv-azureopenia.vault.azure.net/" 
            credential = ClientSecretCredential(
                tenant_id=certificate['tenant_id'],
                client_id=certificate['client_id'],
                client_secret=certificate['client_secret']
            )
            secret_client = SecretClient(vault_url=key_vault_url, credential=credential)
            api_key = secret_client.get_secret('OPEN-AI-KEY').value

            # 3. Configurar Cliente
            print("‚öôÔ∏è Configurando Azure OpenAI (Autom√°tico)...")
            return AzureOpenAI(
                azure_endpoint="https://datasciencellm.openai.azure.com/",
                api_key=api_key,
                api_version="2024-12-01-preview",
            )
            
        except ImportError as e:
            raise ImportError(f"Depend√™ncias ausentes ou erro de importa√ß√£o: {e}. Instale: azure-identity, azure-keyvault-secrets, openai")
        except Exception as e:
            raise RuntimeError(f"Falha na configura√ß√£o autom√°tica do cliente LLM: {e}")

    def _get_fabric_context(self) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Auxiliar para obter o contexto do Fabric (token, workspace_id, notebook_id).
        Usa configura√ß√µes do Spark para IDs, o que √© mais robusto que correspond√™ncia por nome.
        """
        try:
            from notebookutils import mssparkutils
            from pyspark.sql import SparkSession
            
            token = mssparkutils.credentials.getToken("pbi")
            
            spark = SparkSession.getActiveSession()
            if not spark:
                logger.warning("Nenhuma sess√£o Spark ativa encontrada.")
                return None, None, None
                
            workspace_id = spark.conf.get("trident.workspace.id", None)
            notebook_id = spark.conf.get("trident.artifact.id", None)
            
            return token, workspace_id, notebook_id
        except ImportError:
            logger.warning("notebookutils ou pyspark n√£o encontrados. N√£o est√° rodando no Fabric?")
            return None, None, None
        except Exception as e:
            logger.warning(f"Falha ao obter contexto do Fabric: {e}")
            return None, None, None

    def _extract_code_hybrid(self) -> str:
        """
        Extrai c√≥digo usando uma estrat√©gia √† prova de falhas:
        1. Tenta API do Fabric (Estrat√©gia A).
        2. Fallback para hist√≥rico do IPython (Estrat√©gia B).
        """
        # Estrat√©gia A: API
        code = self._extract_via_api()
        if code:
            logger.info("C√≥digo extra√≠do com sucesso via API do Fabric.")
            return code
        
        # Estrat√©gia B: Fallback de Mem√≥ria
        logger.info("Recorrendo √† extra√ß√£o via mem√≥ria (Estrat√©gia B).")
        return self._extract_via_memory()

    def _extract_via_api(self) -> Optional[str]:
        token, workspace_id, notebook_id = self._get_fabric_context()
        if not token or not workspace_id or not notebook_id:
            logger.warning("Contexto do Fabric ausente para extra√ß√£o via API.")
            return None

        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        base_url = "https://api.fabric.microsoft.com/v1"
        
        try:
            # Acesso direto usando IDs da configura√ß√£o do Spark
            def_url = f"{base_url}/workspaces/{workspace_id}/items/{notebook_id}/getDefinition"
            response = requests.post(def_url, headers=headers)
            
            definition_json = {}
            if response.status_code == 200:
                definition_json = response.json()
            elif response.status_code == 202:
                # Loop de polling
                operation_url = response.headers.get("Location") or response.headers.get("Operation-Location")
                retry_after = int(response.headers.get("Retry-After", 2))
                
                if operation_url:
                    for _ in range(10): 
                        time.sleep(retry_after)
                        poll_response = requests.get(operation_url, headers=headers)
                        if poll_response.status_code == 200:
                            definition_json = poll_response.json()
                            break
                        if poll_response.status_code != 202:
                            logger.error(f"Polling falhou: {poll_response.status_code}")
                            return None
            
            if not definition_json:
                logger.error(f"Falha ao obter defini√ß√£o. Status final: {response.status_code}")
                return None

            # Parse da Defini√ß√£o
            parts = definition_json.get('definition', {}).get('parts', [])
            full_code = []
            
            payload = None
            for p in parts:
                if 'ipynb' in p.get('path', '').lower():
                    payload = p.get('payload')
                    break
            if not payload and parts:
                payload = parts[0].get('payload')

            if payload:
                decoded = base64.b64decode(payload).decode('utf-8')
                nb_json = json.loads(decoded)
                
                for cell in nb_json.get('cells', []):
                    if cell.get('cell_type') == 'code':
                        source = "".join(cell.get('source', [])) if isinstance(cell.get('source'), list) else str(cell.get('source'))
                        
                        # Verifica√ß√£o de auto-exclus√£o
                        if any(pattern in source for pattern in self.ignore_patterns):
                            continue
                            
                        full_code.append(source)
            
            return "\n\n".join(full_code)

        except Exception as e:
            logger.error(f"Estrat√©gia A falhou: {e}")
            return None

    def _extract_via_memory(self) -> str:
        """
        Recupera c√©lulas executadas da lista global `In` do IPython usando inspect.
        """
        try:
            # Usa inspect para encontrar o frame do chamador que possui 'In' (hist√≥rico do IPython)
            frame = inspect.currentframe()
            history = None
            
            # Sobe na pilha para encontrar o escopo global do notebook
            while frame:
                if 'In' in frame.f_globals and isinstance(frame.f_globals['In'], list):
                    history = frame.f_globals['In']
                    break
                frame = frame.f_back
            
            if not history:
                # Fallback para __main__ se a caminhada na pilha falhar
                import __main__
                if hasattr(__main__, 'In'):
                    history = __main__.In

            if history:
                valid_cells = []
                for cell in history:
                    if isinstance(cell, str) and cell.strip():
                        # Verifica√ß√£o de auto-exclus√£o
                        if any(pattern in cell for pattern in self.ignore_patterns):
                            continue
                        valid_cells.append(cell)
                return "\n\n".join(valid_cells)
            else:
                logger.warning("Hist√≥rico 'In' n√£o encontrado.")
                return ""
        except Exception as e:
            logger.error(f"Estrat√©gia B falhou: {e}")
            return ""

    def _clean_noise(self, code_string: str) -> str:
        # 1. Remove cabe√ßalhos de Licen√ßa Apache (e potencialmente outros)
        code_string = re.sub(r'(?m)^#\s*Copyright.*(?:\n#.*)*', '', code_string)
        code_string = re.sub(r'(?m)^#\s*Licensed under.*(?:\n#.*)*', '', code_string)
        code_string = re.sub(r'(# Licensed to the Apache[\s\S]*?#\n)', '', code_string)
        
        # 2. Remove blocos init_spark
        pattern_spark = r'def init_spark\(\):[\s\S]*?del init_spark'
        code_string = re.sub(pattern_spark, '', code_string)

        # 3. Remove sc.setJobGroup / sc.setLocalProperty
        code_string = re.sub(r'sc\.setJobGroup\(.*?\)', '', code_string)
        code_string = re.sub(r'sc\.setLocalProperty\(.*?\)', '', code_string)
        code_string = re.sub(r'(sc\.setJobGroup[\s\S]*?sourceId", "default"\))', '', code_string)
        
        # 4. Remove imports e c√≥digo de infraestrutura
        code_string = re.sub(r'(?m)^import notebookutils.*$', '', code_string)
        code_string = re.sub(r'(?m)^from notebookutils.*$', '', code_string)
        code_string = re.sub(r'(import notebookutils|from notebookutils.*|initializeLHContext.*|notebookutils\.prepare.*)', '', code_string)
        
        # 5. Remove comandos M√°gicos
        code_string = re.sub(r'(?m)^%.*$', '', code_string)
        code_string = re.sub(r'(get_ipython\(\)\.run_line_magic.*)', '', code_string)
        
        # 6. Redige segredos (sk-...)
        code_string = re.sub(r'sk-[a-zA-Z0-9]{20,}', 'sk-***REDACTED***', code_string)
        
        # 7. Limpeza final
        code_string = re.sub(r'^[ \t]+$', '', code_string, flags=re.MULTILINE) # Remove linhas vazias com espa√ßos
        code_string = re.sub(r'\n{3,}', '\n\n', code_string) # Compacta newlines excessivos
        
        return code_string.strip()

    def audit_code(self) -> str:
        raw_code = self._extract_code_hybrid()
        clean_code = self._clean_noise(raw_code)
        
        if not clean_code:
            return "Nenhum c√≥digo encontrado para auditar."

        system_prompt = (
'''
# Role: Engenheiro de Dados S√™nior (Microsoft Fabric/Synapse Auditor)

## Contexto e Objetivo
Voc√™ √© a √∫ltima barreira de qualidade antes de um c√≥digo ir para produ√ß√£o. Sua tarefa √© auditar notebooks PySpark projetados para rodar em pipelines orquestrados (Data Factory/Synapse Pipelines) de forma **100% aut√¥noma**.

**Sua mentalidade:**
* **C√©tico:** Assuma que o c√≥digo vai falhar silenciosamente se n√£o for verificado.
* **Orientado a Custos:** Otimiza√ß√£o de CU (Capacity Units) no Fabric √© prioridade.
* **Seguran√ßa Zero Trust:** Nenhuma credencial deve estar exposta.

---

## 1. Diretrizes de Filtragem (Redu√ß√£o de Ru√≠do)
**N√ÉO** aponte problemas nestes casos (salvo se causarem erro expl√≠cito):
* Imports padr√£o (`pyspark.sql.functions`, `types`, etc.), a menos que n√£o utilizados.
* Configura√ß√£o de sess√£o Spark (`spark = ...`), pois o Fabric gerencia isso, mas n√£o √© um erro cr√≠tico.
* Coment√°rios de documenta√ß√£o (docstrings), a menos que revelem l√≥gica insegura.

---

## 2. Regras de Auditoria (Checklist Rigoroso)

### A. Limpeza de Artefatos Interativos (N√≠vel: BLOQUEANTE)
O c√≥digo n√£o pode conter comandos que exijam intera√ß√£o humana ou poluam os logs do driver.
* **Proibido:** `display()`, `df.show()`, `df.printSchema()`, `input()`.
* **Proibido:** Bibliotecas de plotagem (`matplotlib`, `seaborn`, `plotly`).
* **Restrito:** `print()` solto. (Sugerir substitui√ß√£o por `logging` ou remo√ß√£o).

### B. Seguran√ßa e Governan√ßa (N√≠vel: CR√çTICO)
* **Hardcoded Secrets:** Senhas, SAS Tokens, Access Keys ou Connection Strings expl√≠citas.
    * *Solu√ß√£o Obrigat√≥ria:* Usar Azure Key Vault via `mssparkutils.credentials.getSecret()`.
* **Dados Sens√≠veis (PII):** Logs imprimindo dados de clientes (CPF, Email, etc.).

### C. Performance e Otimiza√ß√£o Fabric (N√≠vel: ALTO)
* **Schema Enforcement:** Ingest√£o de API/JSON/CSV sem `schema` definido (risco de infer√™ncia custosa e erro de tipo).
* **Delta Lake Best Practices:**
    * Uso de `MERGE` sem colunas de poda (partition pruning).
    * Falta de `OPTIMIZE` ou `VACUUM` em processos de escrita massiva.
    * Particionamento excessivo em tabelas pequenas (< 1GB).
* **A√ß√µes Coletoras:** Uso inseguro de `.collect()` ou `.toPandas()`.
    * *Regra:* Aceit√°vel apenas para m√©tricas de controle min√∫sculas. Se usado no dataset principal -> **Reprovar**.

### D. Estabilidade e Orquestra√ß√£o (N√≠vel: ALTO)
* **Controle de Fluxo:** Loops `while` sem timeout ou `for` iterando sobre dados massivos (non-vectorized operations).
* **Retorno de Pipeline:** O notebook deve finalizar com `mssparkutils.notebook.exit()` para comunicar status ao orquestrador.
* **Caminhos:** Prefer√™ncia por caminhos OneLake (`abfss://...`) em vez de montagens locais legadas.

### E. Qualidade de C√≥digo (N√≠vel: M√âDIO/DICA)
* **Magic Numbers:** N√∫meros soltos na l√≥gica sem explica√ß√£o ou constante nomeada.
* **Nomenclatura:** Vari√°veis como `df1`, `temp`, `teste`.
* **Tratamento de Erros:** Blocos `try/except` vazios ou gen√©ricos (`except Exception: pass`).

---

## 3. Formato de Sa√≠da Obrigat√≥rio

Para cada problema encontrado, gere um bloco no seguinte padr√£o Markdown:

### üî¥ [BLOQUEANTE / CR√çTICO] ou üü° [ALTO] ou üîµ [DICA]
**Trecho/Linha:** `C√≥digo ou n√∫mero da linha`
**Viola√ß√£o:** Explique qual regra foi quebrada e o impacto (ex: "Isso far√° o log do driver estourar em produ√ß√£o").
**Corre√ß√£o Sugerida:**
```python
# Exemplo de como o c√≥digo deveria ser
'''
)
        
        return self._call_llm(system_prompt, clean_code)

    def summarize_notebook(self) -> str:
        raw_code = self._extract_code_hybrid()
        clean_code = self._clean_noise(raw_code)
        
        if not clean_code:
            return "Nenhum c√≥digo encontrado para resumir."

        system_prompt = (
'''# Role
Voc√™ √© um Engenheiro de Dados S√™nior, especialista em Microsoft Fabric, Delta Lake e orquestra√ß√£o de pipelines complexos.

# Objetivo
Sua tarefa √© analisar o c√≥digo de um notebook do Microsoft Fabric (fornecido a seguir) e gerar uma **Documenta√ß√£o T√©cnica Completa**. A documenta√ß√£o deve ser estruturada, profissional e focar na l√≥gica de neg√≥cios, fluxo de dados e arquitetura t√©cnica.

# Instru√ß√µes de An√°lise
Para realizar a tarefa, voc√™ deve ler e interpretar integralmente o notebook, considerando:
* C√©lulas de c√≥digo (PySpark, Python, SQL).
* Utiliza√ß√£o de bibliotecas espec√≠ficas (`mssparkutils`, `delta`, `pyspark.sql`).
* Coment√°rios, prints, logs e mensagens de erro.
* Chamadas de orquestra√ß√£o (`mssparkutils.notebook.run`, `exit`).

---

# Estrutura Obrigat√≥ria da Documenta√ß√£o
A sa√≠da deve seguir estritamente os t√≥picos abaixo:

## 1. Resumo Executivo
* **Vis√£o Geral:** Uma descri√ß√£o de alto n√≠vel do que o notebook faz.
* **Diagrama Narrativo:** Representa√ß√£o textual do fluxo (ex: `Origem [SAP] -> Processamento [PySpark] -> Destino [Delta Table]`).

## 2. Arquitetura e Fluxo de Dados (End-to-End)
* **Origem dos Dados:**
    * Identifique a fonte (SAP, SQL Server, OneLake, API, Arquivos RAW, etc.).
    * Liste os caminhos (paths) ou tabelas de leitura.
* **Camadas Utilizadas:**
    * Mapeie o movimento dos dados entre camadas (RAW -> BRONZE -> SILVER -> GOLD/WAREHOUSE).
* **Destino e Persist√™ncia:**
    * Tabelas ou arquivos gerados.
    * Formato de escrita (Delta, Parquet, CSV).
    * Modo de escrita (`append`, `overwrite`, `merge`).
    * Estrat√©gia incremental (uso de `watermark`, carimbos de data/hora, chaves como `ID_VDXM`).
    * Otimiza√ß√µes aplicadas (`OPTIMIZE`, `VACUUM`, `PARTITION BY`).

## 3. Detalhe das Transforma√ß√µes e Regras de Neg√≥cio
Para cada etapa l√≥gica do c√≥digo, descreva:
* **Tratamentos:** Casts, normaliza√ß√£o de colunas, limpeza de strings.
* **L√≥gica Relacional:** Joins, uni√µes, deduplica√ß√µes.
* **Filtros:** Regras de exclus√£o ou sele√ß√£o de dados.
* **Regras de Neg√≥cio Espec√≠ficas:** C√°lculos ou l√≥gica complexa aplicada ao dataset.

## 4. Orquestra√ß√£o e Controle de Qualidade
* **Integra√ß√£o com Fabric:** Como o notebook recebe par√¢metros e como retorna status (`mssparkutils.notebook.exit`).
* **Mecanismos de Resili√™ncia:** Blocos `try/except`, valida√ß√£o de paths (`fs.exists`), tratamento de nulos.
* **Logging e Monitoramento:** Como o notebook registra o progresso ou erros (listas acumuladas de erros, prints de controle).

## 5. Dicion√°rio de Estruturas (Tabelas e Vari√°veis)
* Liste as principais tabelas lidas e escritas.
* Indique as chaves prim√°rias ou colunas de parti√ß√£o identificadas.

## 6. Observa√ß√µes e Recomenda√ß√µes (Critical Review)
Como Engenheiro S√™nior, analise o c√≥digo criticamente e liste:
* **Riscos T√©cnicos:** Pontos fr√°geis que podem causar falhas.
* **Performance:** Oportunidades de otimiza√ß√£o (paralelismo, predicate pushdown, z-ordering).
* **Melhores Pr√°ticas:** Sugest√µes para adequar o c√≥digo aos padr√µes do Microsoft Fabric e Delta Lake.

---

**[INSERIR C√ìDIGO DO NOTEBOOK AQUI]**
'''
        )
        
        return self._call_llm(system_prompt, clean_code)

    def _call_llm(self, system_prompt: str, user_content: str) -> str:
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ]
            
            # Verifica se √© um cliente OpenAI (novo padr√£o)
            if hasattr(self.llm_client, 'chat'):
                deployment_name = "Qualificacao_de_JSON" # Nome do deployment fixo conforme solicitado
                response = self.llm_client.chat.completions.create(
                    model=deployment_name,
                    messages=messages,
                    temperature=0.01,
                    max_tokens=3000,
                )
                return response.choices[0].message.content

            # Fallback para LangChain (caso o usu√°rio tenha passado um cliente customizado antigo)
            elif hasattr(self.llm_client, 'invoke') or hasattr(self.llm_client, '__call__'):
                from langchain.schema import HumanMessage, SystemMessage
                lc_messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_content)
                ]
                if hasattr(self.llm_client, 'invoke'):
                    response = self.llm_client.invoke(lc_messages)
                else:
                    response = self.llm_client(lc_messages)
                return getattr(response, 'content', str(response))
            
            else:
                return "Erro: Cliente LLM n√£o reconhecido."
            
        except Exception as e:
            return f"Chamada ao LLM Falhou: {e}"
